{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6850669-0a56-4d54-a410-f48e9f3aef71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b0cb1-b567-4f70-bdb4-cc910ac844da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mp_api.client import MPRester\n",
    "from emmet.core.summary import HasProps, summary_fields\n",
    "\n",
    "API = \"05pwL0aLyXiEGNsZRFhKcju39mwEqbz8\"\n",
    "\n",
    "necessary_field = ['material_id', 'formula_pretty','formula_anonymous', 'structure']\n",
    "wanted_field = summary_fields['thermo']+summary_fields['dielectric']\n",
    "wanted_properties = [HasProps.dielectric]\n",
    "\n",
    "with MPRester(API) as mpr:\n",
    "    docs = mpr.materials.summary.search(\n",
    "        has_props = wanted_properties, fields=necessary_field+wanted_field\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef9200-f6f5-4d68-bd1e-6d49d9b10f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Converting the data raw data into dataframe\n",
    "materials = []\n",
    "for material in docs:\n",
    "  material = dict(material)\n",
    "  materials.append(material)\n",
    "df = pd.DataFrame(materials)\n",
    "df = df[necessary_field+wanted_field]\n",
    "del df['decomposes_to']\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8d806-b329-4a09-812c-ca0ce4bb9fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7052239b-4214-4d89-be4c-f4982386e900",
   "metadata": {},
   "source": [
    "## Saving data and importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ac5a0-6cb6-49a7-a50e-6a93e1fb08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.io.cif import CifWriter\n",
    "structure_cif = []\n",
    "for index, row in df.iterrows():\n",
    "    structure = row[\"structure\"]\n",
    "    cif_writer = CifWriter(structure)\n",
    "    cif_string = cif_writer.__str__()\n",
    "    structure_cif.append(cif_string)\n",
    "\n",
    "df['structure']=structure_cif\n",
    "\n",
    "filepath = \"Database\"\n",
    "filename = \"DataBase.xlsx\"\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel(filepath+\"/\"+filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38854948-b49c-42d3-a64d-905adc17d6f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "```\n",
    "from pymatgen.io.cif import CifParser\n",
    "\n",
    "\n",
    "filepath = \"<Fill your file path>\"\n",
    "filename = \"<Fill your file name>.csv\"\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df = pd.read_csv(filepath+\"/\"+filename)\n",
    "\n",
    "# Function to convert CIF string to PyMatGen structure\n",
    "def cif_to_structure(cif_string):\n",
    "    try:\n",
    "        parser = CifParser.from_string(cif_string)\n",
    "        structure = parser.get_structures()[0]  # Assuming there's only one structure in the CIF\n",
    "        return structure\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create a new 'structure' column\n",
    "df['structure                  n                                  \n",
    "                                                                  \n",
    "                                                                   '] = df['structure'].apply(cif_to_structure)\n",
    "\n",
    "# Print the DataFrame with the new 'structure' column\n",
    "df.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f2237-6d0d-464e-84de-3d3875cdeec5",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ef8de-7d83-467e-a74c-fcf689549a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('./Database/DataBase.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002cbcb8-b7c3-4cca-a923-e5043f03ed11",
   "metadata": {},
   "source": [
    "# Applied Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6042d0c4-1c01-40fd-b602-78b0cbd0efa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Str to composition descriptor\n",
    "from matminer.featurizers.conversions import StrToComposition\n",
    "\n",
    "df = StrToComposition().featurize_dataframe(df, \"formula_pretty\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017d06b1-a91a-4844-9211-ee59efcb4091",
   "metadata": {},
   "source": [
    "# Formula Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb277ddc-46ee-4bea-bf7c-15d87fd24c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element Fraction Descriptor\n",
    "from matminer.featurizers.composition import ElementFraction\n",
    "\n",
    "df_fraction = df.copy()\n",
    "df_fraction = ElementFraction().featurize_dataframe(df_fraction, col_id=\"composition\") # input the \"composition\" column to the featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145cd55-d0c7-40c5-a73a-3f8ae7725b81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_fraction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5423a-62b6-4226-a741-36ecaa170ba8",
   "metadata": {},
   "source": [
    "# Element Property Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8044e5ad-e965-4393-99e8-ec9d212dc523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matminer.featurizers.composition import ElementProperty\n",
    "\n",
    "df_el_prop = df.copy()\n",
    "\n",
    "ep_feat = ElementProperty.from_preset(preset_name=\"magpie\")\n",
    "df_el_prop = ep_feat.featurize_dataframe(df_el_prop, col_id=\"composition\")  # input the \"composition\" column to the featurizer\n",
    "df_el_prop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b99a5d-4d5a-44e2-abfb-b5baa22cc464",
   "metadata": {},
   "source": [
    "# Preparing data for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99818171-a491-4987-8a78-29e201aba23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = df_fraction['formation_energy_per_atom']\n",
    "excluded = np.array(df.columns)\n",
    "\n",
    "X_el = df_el_prop.drop(excluded, axis=1)\n",
    "X_frac = df_fraction.drop(excluded, axis=1)\n",
    "\n",
    "_, feat_col_el = X_el.shape\n",
    "_, feat_col_frac = X_frac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1d09f4-ed76-424b-aaf2-3970d476845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "el = np.array(X_el.columns.values)\n",
    "frac = np.array(X_frac.columns.values)\n",
    "\n",
    "np.save(\"TrainingData/el_col.npy\", el)\n",
    "np.save(\"TrainingData/frac_col.npy\", frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfafdd18-b851-4e6d-8625-af80de8ac48e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"There are {} possible descriptors:\\n\\n{}\".format(X_el.shape[1], X_el.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1460f39f-4026-4faa-928b-2fdf3701beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} possible descriptors:\\n\\n{}\".format(X_frac.shape[1], X_frac.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e750a459-51ad-49fc-bc84-439b6c5b4311",
   "metadata": {},
   "source": [
    "# Saving training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3d242-7a11-41ac-b632-e10e7f816688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk for later training\n",
    "np.save(\"TrainingData/X_el.npy\", X_el)\n",
    "np.save(\"TrainingData/X_frac.npy\", X_frac)\n",
    "np.save(\"TrainingData/y.npy\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b707bb38-08e9-4511-ac15-b8a2ee7c48f9",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9771c98-5da6-465f-9355-ebb87341d6ca",
   "metadata": {},
   "source": [
    "## Element Property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741b27c-e309-4dbd-8dcd-a3f288f73af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data\n",
    "X_el = np.load(\"TrainingData/X_el.npy\")\n",
    "y = np.load(\"TrainingData/y.npy\")\n",
    "\n",
    "el = np.load(\"TrainingData/el_col.npy\", allow_pickle=True) # allow_pickel = True when you input string data\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_el, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d4141-b3e7-45b2-a025-d35f636da3ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train an XGBoost model\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85133324-dddf-49d2-a1fb-e4d98f7e3ae7",
   "metadata": {},
   "source": [
    "### Saving and uploading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4fdcb5-0b11-450c-ab26-a7343d164d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pickle\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4c309a-4505-41e2-b76f-9dbb5815cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file using pickle\n",
    "with open(\"Models/el_xgboost.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f2117-6bde-4c61-a7df-00206fceef3d",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589964b1-b694-422a-8e60-57da7a6382c0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model from the saved file\n",
    "with open(\"Models/el_xgboost.pkl\", \"rb\") as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "    \n",
    "# Now you can use the loaded model for predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE) as a measure of model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_text = f\"Mean Squared Error: {mse:.2f}\"\n",
    "\n",
    "# Create a graph for prediction vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Prediction vs. Actual Values\")\n",
    "\n",
    "# Add the MSE value as text in the graph\n",
    "plt.text(0.1, 0.9, mse_text, transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.1, 0.7, \"Element Property Magpiedata\", transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.savefig(\"Images/el_xgboost.png\")  # Save the plot to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15240ef-a39d-48d4-81e2-ad1fb3d75969",
   "metadata": {},
   "source": [
    "### Adding R2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54877c6-3ca0-4957-976e-bb8a26620c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model from the saved file\n",
    "with open(\"Models/el_xgboost.pkl\", \"rb\") as model_file:\n",
    "   model = pickle.load(model_file)\n",
    "   \n",
    "# Now you can use the loaded model for predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE) as a measure of model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_text = f\"Mean Squared Error: {mse:.2f}\"\n",
    "\n",
    "# Calculate the R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r2_text = f\"R2 Score: {r2:.2f}\"\n",
    "\n",
    "# Create a graph for prediction vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Prediction vs. Actual Values\")\n",
    "\n",
    "# Add the MSE and R2 values as text in the graph\n",
    "plt.text(0.1, 0.9, mse_text, transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.1, 0.8, r2_text, transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.1, 0.7, \"Element Property Magpiedata\", transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.savefig(\"Images/el_xgboost_r2.png\") # Save the plot to a file\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992416e-60a3-4274-8162-da265f5384fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model from the saved file\n",
    "with open(\"Models/el_xgboost.pkl\", \"rb\") as model_file:\n",
    "  model = pickle.load(model_file)\n",
    "  \n",
    "# Now you can use the loaded model for predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE) as a measure of model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_text = f\"Mean Squared Error: {mse:.2f}\"\n",
    "\n",
    "# Calculate the R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r2_text = f\"R2 Score: {r2:.2f}\"\n",
    "\n",
    "# Calculate the slope and intercept of the line that best fits the data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(y_test.reshape(-1, 1), y_pred)\n",
    "slope, intercept = lr.coef_[0], lr.intercept_\n",
    "\n",
    "# Calculate the absolute differences\n",
    "diff = np.abs(y_test - y_pred)\n",
    "\n",
    "# Create a colormap and use it to map the absolute differences to colors\n",
    "cmap = plt.cm.get_cmap('viridis')\n",
    "colors = cmap(diff / np.max(diff))\n",
    "\n",
    "# Create a graph for prediction vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, c=colors, alpha=0.5)\n",
    "plt.plot(y_test, slope * y_test + intercept, linestyle='dashed')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Prediction vs. Actual Values\")\n",
    "\n",
    "# Add the MSE and R2 values as text in the graph\n",
    "plt.text(0.1, 0.9, mse_text, transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.1, 0.8, r2_text, transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.1, 0.7, \"Element Property Magpiedata\", transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.savefig(\"Images/el_xgboost.png\") # Save the plot to a file\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa23ed5-a083-4ce8-a67b-dbe732572737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the saved file\n",
    "with open(\"Models/el_xgboost.pkl\", \"rb\") as model_file:\n",
    " model = pickle.load(model_file)\n",
    " \n",
    "# Now you can use the loaded model for predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE) as a measure of model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_text = f\"Mean Squared Error: {mse:.2f}\"\n",
    "\n",
    "# Calculate the R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r2_text = f\"R2 Score: {r2:.2f}\"\n",
    "\n",
    "# Calculate the slope and intercept of the line that best fits the data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(y_test.reshape(-1, 1), y_pred)\n",
    "slope, intercept = lr.coef_[0], lr.intercept_\n",
    "\n",
    "# Calculate the absolute differences\n",
    "diff = np.abs(y_test - y_pred)\n",
    "\n",
    "# Normalize the absolute differences\n",
    "norm = plt.Normalize(vmin=0, vmax=np.max(diff))\n",
    "\n",
    "# Create a colormap and use it to map the absolute differences to colors\n",
    "cmap = plt.cm.get_cmap('hot')\n",
    "colors = cmap(norm(diff))\n",
    "\n",
    "# Create a graph for prediction vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, c=colors, alpha=0.5)\n",
    "plt.plot(y_test, slope * y_test + intercept, linestyle='dashed')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Prediction vs. Actual Values\")\n",
    "\n",
    "# Add the MSE and R2 values as text in the graph\n",
    "plt.text(0.05, 0.7, mse_text, transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.05, 0.8, r2_text, transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.05, 0.9, \"Element Property Magpiedata\", transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "# Create a colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap='hot', norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm)\n",
    "cbar.ax.set_ylabel('|Actual - Predicted|', rotation=270, fontsize=15, labelpad=15)\n",
    "\n",
    "plt.savefig(\"Images/el_xgboost.png\") # Save the plot to a file\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e44964-2e28-4a50-81d9-3b477a28db74",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f26c2c-57b7-4002-9b62-2e2bc4c38b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Get feature names (assuming your feature names are stored in a list)\n",
    "feature_names = el\n",
    "\n",
    "# Sort the feature importances and select the top ten\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "top_ten_indices = sorted_idx[:10]\n",
    "top_ten_importances = importances[top_ten_indices]\n",
    "top_ten_feature_names = [feature_names[i] for i in top_ten_indices]\n",
    "\n",
    "# Create a bar graph for feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_ten_feature_names, top_ten_importances)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.title(\"Top Ten Feature Importance\")\n",
    "plt.gca().invert_yaxis()  # Invert the y-axis to display the most important feature at the top\n",
    "plt.savefig(\"Images/el_Xgboost_feat.png\")  # Save the plot to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08fda51-7582-4469-9d17-765dd0d8652a",
   "metadata": {},
   "source": [
    "## Element Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9492e4c6-32df-4486-a59b-68c3c20f7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data\n",
    "X_frac = np.load(\"TrainingData/X_frac.npy\")\n",
    "y = np.load(\"TrainingData/y.npy\")\n",
    "\n",
    "frac = np.load(\"TrainingData/frac_col.npy\", allow_pickle=True) # allow_pickel = True when you input string data\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_frac, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92934371-ae93-495b-8919-ae1983b32397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train an XGBoost model\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a0399-10b1-4785-b55e-acc2e1c40cb3",
   "metadata": {},
   "source": [
    "### Saving and uploading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbbbf62-f1c2-403e-81fb-efd5b23929de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "with open(\"Models/frac_xgboost.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807d6b0-f7fa-4025-a2fb-f426078cbc2c",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674ef98-2ac1-464f-8ba9-64d9aadae6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the saved file\n",
    "with open(\"Models/frac_xgboost.pkl\", \"rb\") as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "    \n",
    "# Now you can use the loaded model for predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE) as a measure of model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_text = f\"Mean Squared Error: {mse:.2f}\"\n",
    "\n",
    "# Create a graph for prediction vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Prediction vs. Actual Values\")\n",
    "\n",
    "# Add the MSE value as text in the graph\n",
    "plt.text(0.1, 0.9, mse_text, transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.savefig(\"Images/frac_xgboost.png\")  # Save the plot to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6559809-e80a-459e-858b-69d9983ae54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the saved file\n",
    "with open(\"Models/frac_xgboost.pkl\", \"rb\") as model_file:\n",
    " model = pickle.load(model_file)\n",
    " \n",
    "# Now you can use the loaded model for predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE) as a measure of model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_text = f\"Mean Squared Error: {mse:.2f}\"\n",
    "\n",
    "# Calculate the R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r2_text = f\"R2 Score: {r2:.2f}\"\n",
    "\n",
    "# Calculate the slope and intercept of the line that best fits the data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(y_test.reshape(-1, 1), y_pred)\n",
    "slope, intercept = lr.coef_[0], lr.intercept_\n",
    "\n",
    "# Calculate the absolute differences\n",
    "diff = np.abs(y_test - y_pred)\n",
    "\n",
    "# Normalize the absolute differences\n",
    "norm = plt.Normalize(vmin=0, vmax=np.max(diff))\n",
    "\n",
    "# Create a colormap and use it to map the absolute differences to colors\n",
    "cmap = plt.cm.get_cmap('hot')\n",
    "colors = cmap(norm(diff))\n",
    "\n",
    "# Create a graph for prediction vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, c=colors, alpha=0.5)\n",
    "plt.plot(y_test, slope * y_test + intercept, linestyle='dashed')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Prediction vs. Actual Values\")\n",
    "\n",
    "# Add the MSE and R2 values as text in the graph\n",
    "plt.text(0.05, 0.7, mse_text, transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.05, 0.8, r2_text, transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.05, 0.9, \"Element Fraction\", transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "# Create a colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap='hot', norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm)\n",
    "cbar.ax.set_ylabel('|Actual - Predicted|', rotation=270, fontsize=15, labelpad=15)\n",
    "\n",
    "plt.savefig(\"Images/frac_xgboost.png\") # Save the plot to a file\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb42ea-d33a-45bf-b8b8-a7b27fbfb340",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a67923-98bc-46b4-bac1-32297e3eb77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Get feature names (assuming your feature names are stored in a list)\n",
    "feature_names = frac\n",
    "\n",
    "# Sort the feature importances and select the top ten\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "top_ten_indices = sorted_idx[:10]\n",
    "top_ten_importances = importances[top_ten_indices]\n",
    "top_ten_feature_names = [feature_names[i] for i in top_ten_indices]\n",
    "\n",
    "# Create a bar graph for feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_ten_feature_names, top_ten_importances)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.title(\"Top Ten Feature Importance\")\n",
    "plt.gca().invert_yaxis()  # Invert the y-axis to display the most important feature at the top\n",
    "plt.savefig(\"Images/frac_Xgboost_feat.png\")  # Save the plot to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a69701-8ffb-4054-97a1-d1ddcff0007c",
   "metadata": {},
   "source": [
    "## DFT Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a628f9-aad5-46eb-99ee-73947198c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "X_dft_atom = np.load(\"TrainingData/X_dft_atom.npy\")\n",
    "y = np.load(\"TrainingData/y_dft.npy\")\n",
    "_, feat_col_dft = X_dft_atom.shape\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dft_atom, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a84aa95-0681-4a03-adac-0423cfecee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Train an XGBoost model\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986c389-337b-4f14-97d7-9d4ca71325b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "with open(\"Models/dft_xgboost.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cddb32-6450-454c-884a-a136c4196b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the saved file\n",
    "with open(\"Models/dft_xgboost.pkl\", \"rb\") as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "    \n",
    "# Now you can use the loaded model for predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE) as a measure of model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_text = f\"Mean Squared Error: {mse:.2f}\"\n",
    "\n",
    "# Create a graph for prediction vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Prediction vs. Actual Values\")\n",
    "\n",
    "# Add the MSE value as text in the graph\n",
    "plt.text(0.1, 0.9, mse_text, transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.savefig(\"Images/dft_xgboost.png\")  # Save the plot to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d59617a-1c96-4a36-ae60-edd5b1604b30",
   "metadata": {},
   "source": [
    "# DFT Based Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbbf3cc-f641-4665-a560-323b4c8cfe29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dft = pd.read_excel(\"Database/DFTBased_Descriptors.xlsx\")\n",
    "df_dft = df_dft.dropna()\n",
    "df_dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8032226-634c-49b3-92b6-5161fd6884c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dft.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86599843-7784-4b7b-92b5-7f3eaa61b3e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dft.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce3eef-3855-41a2-a9e0-ed1a5f99d11f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key, val in enumerate(df_dft.columns):\n",
    "    print(f\"{key=}, {val=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d1f01-1ca0-408e-8af1-ccd72f02e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_dft['formation_energy_per_atom']\n",
    "excluded = np.array(df_dft.columns[:21])\n",
    "\n",
    "X_dft_atom = df_dft.drop(excluded, axis=1)\n",
    "\n",
    "_, feat_col_dft = X_dft_atom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b20f0d-c1e6-4ebf-b98d-9d370a24826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk for later training\n",
    "np.save(\"TrainingData/X_dft_atom.npy\", X_dft_atom)\n",
    "np.save(\"TrainingData/y_dft.npy\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d30c27-c638-44d6-a933-74333bf3d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "X_dft_atom = np.load(\"TrainingData/X_dft_atom.npy\")\n",
    "y = np.load(\"TrainingData/y_dft.npy\")\n",
    "_, feat_col_dft = X_dft_atom.shape\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dft_atom, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199868aa-86f6-4f2f-9851-cc75e259fa85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the input layer with the appropriate input shape\n",
    "input_layer = tf.keras.layers.Input(shape=(feat_col_dft,))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        input_layer,\n",
    "        tf.keras.layers.Dense(feat_col_dft),\n",
    "        tf.keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "history = model.fit(\n",
    "    np.array(X_train),\n",
    "    np.array(y_train),\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)],\n",
    "    batch_size=32\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e9b8b-052b-4104-87d4-44bb2bdd20dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the input layer with the appropriate input shape\n",
    "input_layer = tf.keras.layers.Input(shape=(feat_col_dft,))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        input_layer,\n",
    "        tf.keras.layers.Dense(128, activation='linear'),\n",
    "        tf.keras.layers.Dense(64, activation='linear'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "# # Add L2 regularization to the model\n",
    "# model.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "# model.add(tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "\n",
    "model.compile(loss='mean_squared_error',  # Switched to mean squared error\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "history = model.fit(\n",
    "    np.array(X_train),\n",
    "    np.array(y_train),\n",
    "    epochs=100,  # Increased the number of epochs\n",
    "    verbose=1,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)],  # Increased patience\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62627443-12e5-47a8-90e9-722596777ada",
   "metadata": {},
   "source": [
    "## One Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffdeb1-5738-49b8-8eff-03a3a8558f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_mult = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "results = {}\n",
    "\n",
    "def build_and_train_model(num_mult):\n",
    "\n",
    "    input_layer = tf.keras.layers.Input(shape=(feat_col_dft,))\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "            input_layer,\n",
    "            tf.keras.layers.Dense(int(feat_col_dft*num_mult)),\n",
    "            tf.keras.layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=\"adam\")\n",
    "\n",
    "    history = model.fit(\n",
    "        np.array(X_train),\n",
    "        np.array(y_train),\n",
    "        epochs=10,\n",
    "        verbose = 0,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)],\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    loss = history.history['val_loss'][-1]\n",
    "\n",
    "    return loss\n",
    "\n",
    "for num_mult in neuron_mult: # adding iteration\n",
    "    print(f\"Number of neuron: {feat_col_dft*num_mult}\")\n",
    "    loss = build_and_train_model(num_mult)\n",
    "    print(f\"Loss value: {loss}\")\n",
    "    print(\"-\"*50)\n",
    "    results[num_mult] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb5caa-bce1-4a48-ad9d-1aef77ed627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a JSON file\n",
    "# filepath = \"<file path>\"\n",
    "import json\n",
    "\n",
    "filename = \"OneLayer.json\"\n",
    "\n",
    "with open('DeepLearning'+'/'+filename, 'w') as json_file:\n",
    "    json.dump(results, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d02ed-6563-4200-b98e-d9d6f38dd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results from the JSON file\n",
    "with open('DeepLearning'+'/'+filename, 'r') as json_file:\n",
    "    results = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bbf863-3ce9-42c4-867c-6f9256cd845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the values for the x-axis and y-axis from the results dictionary\n",
    "x_values = [num for num in neuron_mult]\n",
    "y_values = [results[str(num)] for num in neuron_mult]\n",
    "\n",
    "# Plot the loss vs. number of neurons\n",
    "plt.plot(x_values, y_values, marker='o')\n",
    "plt.xlabel(\"Number of Neurons\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title(\"Loss vs. Number of Neurons\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Save or display the plot\n",
    "plt.savefig('DeepLearning'+'/'+\"OneLayer.png\")  # Save the plot to a file\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fed535-8105-458e-9db4-82f9091c21d7",
   "metadata": {},
   "source": [
    "## Two layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa37db1-9d7d-49ea-a8de-c25d2a4f6d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_mult = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "results = {}\n",
    "\n",
    "def build_and_train_model(num_mult):\n",
    "\n",
    "    input_layer = tf.keras.layers.Input(shape=(feat_col_dft,))\n",
    "\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer.adapt(np.array(X_dft_atom))\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "            input_layer,\n",
    "            normalizer,\n",
    "            tf.keras.layers.Dense(int(feat_col_dft, activation='relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=\"adam\")\n",
    "\n",
    "    history = model.fit(\n",
    "        np.array(X_train),\n",
    "        np.array(y_train),\n",
    "        epochs=10,\n",
    "        verbose = 0,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)],\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    loss = history.history['val_loss'][-1]\n",
    "\n",
    "    return loss\n",
    "\n",
    "for num_mult in neuron_mult: # adding iteration\n",
    "    print(f\"Number of neuron: {feat_col_dft*num_mult}\")\n",
    "    loss = build_and_train_model(num_mult)\n",
    "    print(f\"Loss value: {loss}\")\n",
    "    print(\"-\"*50)\n",
    "    results[num_mult] = loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
