{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d98e36-0ddb-475b-8bd4-f9bf745e7f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mp_api.client import MPRester\n",
    "from emmet.core.summary import HasProps, summary_fields\n",
    "\n",
    "API = \"05pwL0aLyXiEGNsZRFhKcju39mwEqbz8\"\n",
    "\n",
    "necessary_field = ['material_id', 'formula_pretty','formula_anonymous', 'structure']\n",
    "wanted_field = summary_fields['thermo']+summary_fields['dielectric']\n",
    "wanted_properties = [HasProps.dielectric]\n",
    "\n",
    "with MPRester(API) as mpr:\n",
    "    docs = mpr.materials.summary.search(\n",
    "        has_props = wanted_properties, fields=necessary_field+wanted_field\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037cd0ca-9331-4697-acaa-c64f12249c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Converting the data raw data into dataframe\n",
    "materials = []\n",
    "for material in docs:\n",
    "  material = dict(material)\n",
    "  materials.append(material)\n",
    "df = pd.DataFrame(materials)\n",
    "df = df[necessary_field+wanted_field]\n",
    "del df['decomposes_to']\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0606b93f-1243-4b10-bc27-732582f1822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77c8593-d0c3-4479-81e8-30c9065b4100",
   "metadata": {},
   "source": [
    "## Saving data and importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94f3852-3283-4fd9-96cc-361c938b0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.io.cif import CifWriter\n",
    "structure_cif = []\n",
    "for index, row in df.iterrows():\n",
    "    structure = row[\"structure\"]\n",
    "    cif_writer = CifWriter(structure)\n",
    "    cif_string = cif_writer.__str__()\n",
    "    structure_cif.append(cif_string)\n",
    "\n",
    "df['structure']=structure_cif\n",
    "\n",
    "filepath = \"Database\"\n",
    "filename = \"DataBase.xlsx\"\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel(filepath+\"/\"+filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb811e89-f4a1-469b-8d18-fce66ec22c49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "```\n",
    "from pymatgen.io.cif import CifParser\n",
    "\n",
    "\n",
    "filepath = \"<Fill your file path>\"\n",
    "filename = \"<Fill your file name>.csv\"\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df = pd.read_csv(filepath+\"/\"+filename)\n",
    "\n",
    "# Function to convert CIF string to PyMatGen structure\n",
    "def cif_to_structure(cif_string):\n",
    "    try:\n",
    "        parser = CifParser.from_string(cif_string)\n",
    "        structure = parser.get_structures()[0]  # Assuming there's only one structure in the CIF\n",
    "        return structure\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create a new 'structure' column\n",
    "df['structure                  n                                  \n",
    "                                                                  \n",
    "                                                                   '] = df['structure'].apply(cif_to_structure)\n",
    "\n",
    "# Print the DataFrame with the new 'structure' column\n",
    "df.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce8bd7-d369-4e9b-bb32-cdea53c08ff4",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e6bd3-0154-45b8-a315-9224e2e31802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('./Database/DataBase.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c6cfa-4a1c-4544-ae14-a7e539bb053b",
   "metadata": {},
   "source": [
    "# Applied Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf469e0-3e2c-48a5-86b2-f72607cb54ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Str to composition descriptor\n",
    "from matminer.featurizers.conversions import StrToComposition\n",
    "\n",
    "df = StrToComposition().featurize_dataframe(df, \"formula_pretty\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4013cb1f-92d7-451f-960f-72364375b5db",
   "metadata": {},
   "source": [
    "# Formula Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c6494-322a-425e-83eb-e787a8358cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element Fraction Descriptor\n",
    "from matminer.featurizers.composition import ElementFraction\n",
    "\n",
    "df_fraction = df.copy()\n",
    "df_fraction = ElementFraction().featurize_dataframe(df_fraction, col_id=\"composition\") # input the \"composition\" column to the featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb178f6-2e81-45e9-bdcd-5fbd410d81e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_fraction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43baed6-a3b7-4a81-891a-9c173f1e23a0",
   "metadata": {},
   "source": [
    "# Element Property Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a0742-6207-4e54-9bda-ce31e41f0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matminer.featurizers.composition import ElementProperty\n",
    "\n",
    "df_el_prop = df.copy()\n",
    "\n",
    "ep_feat = ElementProperty.from_preset(preset_name=\"magpie\")\n",
    "df_el_prop = ep_feat.featurize_dataframe(df_el_prop, col_id=\"composition\")  # input the \"composition\" column to the featurizer\n",
    "df_el_prop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998dd21-13f5-44ac-a819-c47fbefbb7f8",
   "metadata": {},
   "source": [
    "# Preparing data for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2a368-6a71-4920-86aa-b65f236159c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = df_fraction['formation_energy_per_atom']\n",
    "excluded = np.array(df.columns)\n",
    "\n",
    "X_el = df_el_prop.drop(excluded, axis=1)\n",
    "X_frac = df_fraction.drop(excluded, axis=1)\n",
    "\n",
    "_, feat_col_el = X_el.shape\n",
    "_, feat_col_frac = X_frac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a691aa-5764-436e-b2ca-ef76ef25d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "el = np.array(X_el.columns.values)\n",
    "frac = np.array(X_frac.columns.values)\n",
    "\n",
    "np.save(\"TrainingData/el_col.npy\", el)\n",
    "np.save(\"TrainingData/frac_col.npy\", frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ecaf5-742f-4610-b73b-2ed03c589c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"There are {} possible descriptors:\\n\\n{}\".format(X_el.shape[1], X_el.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae81a6-98fb-4e59-84a0-0bbfdb3fb7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} possible descriptors:\\n\\n{}\".format(X_frac.shape[1], X_frac.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0a529-16e1-4561-a926-b747af8c6679",
   "metadata": {},
   "source": [
    "# Saving training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b08b0-6d6e-4c1b-9926-6b1b381c66c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk for later training\n",
    "np.save(\"TrainingData/X_el.npy\", X_el)\n",
    "np.save(\"TrainingData/X_frac.npy\", X_frac)\n",
    "np.save(\"TrainingData/y.npy\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a2c41-6461-4cc3-9d95-0ac4352cc4db",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b713975-9bc2-465b-adb2-9b896df27a69",
   "metadata": {},
   "source": [
    "## Element Property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b47cc-78dd-4252-a7a8-44d94cadf831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data\n",
    "X_el = np.load(\"TrainingData/X_el.npy\")\n",
    "y = np.load(\"TrainingData/y.npy\")\n",
    "\n",
    "el = np.load(\"TrainingData/el_col.npy\", allow_pickle=True) # allow_pickel = True when you input string data\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_el, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925dbf4-d0ec-4473-b3ec-fa155a310129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train an XGBoost model\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c29c5-3411-4914-a604-9fafbf01476a",
   "metadata": {},
   "source": [
    "### Saving and uploading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c3d7e-66b4-4b39-a302-8ccecd21b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c4720-cc39-4d26-88ae-f31d3fde1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file using pickle\n",
    "with open(\"Models/el_xgboost.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0ea16-9e92-4378-b815-280b41cada65",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4e6ebd-b4bc-4b9c-9de2-362fa36a401f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model from the saved file\n",
    "with open(\"Models/el_xgboost.pkl\", \"rb\") as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "    \n",
    "# Now you can use the loaded model for predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE) as a measure of model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_text = f\"Mean Squared Error: {mse:.2f}\"\n",
    "\n",
    "# Create a graph for prediction vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Prediction vs. Actual Values\")\n",
    "\n",
    "# Add the MSE value as text in the graph\n",
    "plt.text(0.1, 0.9, mse_text, transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.1, 0.7, \"Element Property Magpiedata\", transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.savefig(\"Images/el_xgboost.png\")  # Save the plot to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ee76f-965d-4f36-8d97-e2dd6d68c742",
   "metadata": {},
   "source": [
    "### Adding R2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b60be7b-bff1-4190-bc39-bc285d197129",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the model from the saved file\n",
    "with open(\"Models/el_xgboost.pkl\", \"rb\") as model_file:\n",
    "   model = pickle.load(model_file)\n",
    "   \n",
    "# Now you can use the loaded model for predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE) as a measure of model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_text = f\"Mean Squared Error: {mse:.2f}\"\n",
    "\n",
    "# Calculate the R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r2_text = f\"R2 Score: {r2:.2f}\"\n",
    "\n",
    "# Create a graph for prediction vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Prediction vs. Actual Values\")\n",
    "\n",
    "# Add the MSE and R2 values as text in the graph\n",
    "plt.text(0.1, 0.9, mse_text, transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.1, 0.8, r2_text, transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.1, 0.7, \"Element Property Magpiedata\", transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.savefig(\"Images/el_xgboost_r2.png\") # Save the plot to a file\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041219c3-f237-4ec8-9844-d2636b3c312a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model from the saved file\n",
    "with open(\"Models/el_xgboost.pkl\", \"rb\") as model_file:\n",
    "  model = pickle.load(model_file)\n",
    "  \n",
    "# Now you can use the loaded model for predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE) as a measure of model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_text = f\"Mean Squared Error: {mse:.2f}\"\n",
    "\n",
    "# Calculate the R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r2_text = f\"R2 Score: {r2:.2f}\"\n",
    "\n",
    "# Calculate the slope and intercept of the line that best fits the data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(y_test.reshape(-1, 1), y_pred)\n",
    "slope, intercept = lr.coef_[0], lr.intercept_\n",
    "\n",
    "# Calculate the absolute differences\n",
    "diff = np.abs(y_test - y_pred)\n",
    "\n",
    "# Create a colormap and use it to map the absolute differences to colors\n",
    "cmap = plt.cm.get_cmap('viridis')\n",
    "colors = cmap(diff / np.max(diff))\n",
    "\n",
    "# Create a graph for prediction vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, c=colors, alpha=0.5)\n",
    "plt.plot(y_test, slope * y_test + intercept, linestyle='dashed')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Prediction vs. Actual Values\")\n",
    "\n",
    "# Add the MSE and R2 values as text in the graph\n",
    "plt.text(0.1, 0.9, mse_text, transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.1, 0.8, r2_text, transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.1, 0.7, \"Element Property Magpiedata\", transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.savefig(\"Images/el_xgboost.png\") # Save the plot to a file\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41727506-bdad-438b-bf19-ac291baaa60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the saved file\n",
    "with open(\"Models/el_xgboost.pkl\", \"rb\") as model_file:\n",
    " model = pickle.load(model_file)\n",
    " \n",
    "# Now you can use the loaded model for predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE) as a measure of model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_text = f\"Mean Squared Error: {mse:.2f}\"\n",
    "\n",
    "# Calculate the R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r2_text = f\"R2 Score: {r2:.2f}\"\n",
    "\n",
    "# Calculate the slope and intercept of the line that best fits the data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(y_test.reshape(-1, 1), y_pred)\n",
    "slope, intercept = lr.coef_[0], lr.intercept_\n",
    "\n",
    "# Calculate the absolute differences\n",
    "diff = np.abs(y_test - y_pred)\n",
    "\n",
    "# Normalize the absolute differences\n",
    "norm = plt.Normalize(vmin=0, vmax=np.max(diff))\n",
    "\n",
    "# Create a colormap and use it to map the absolute differences to colors\n",
    "cmap = plt.cm.get_cmap('hot')\n",
    "colors = cmap(norm(diff))\n",
    "\n",
    "# Create a graph for prediction vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, c=colors, alpha=0.5)\n",
    "plt.plot(y_test, slope * y_test + intercept, linestyle='dashed')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Prediction vs. Actual Values\")\n",
    "\n",
    "# Add the MSE and R2 values as text in the graph\n",
    "plt.text(0.05, 0.7, mse_text, transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.05, 0.8, r2_text, transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.05, 0.9, \"Element Property Magpiedata\", transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "# Create a colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap='hot', norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm)\n",
    "cbar.ax.set_ylabel('|Actual - Predicted|', rotation=270, fontsize=15, labelpad=15)\n",
    "\n",
    "plt.savefig(\"Images/el_xgboost.png\") # Save the plot to a file\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff3755-bdc2-416e-9814-7232853f111a",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa71d06-7c49-476b-b21f-5ac2fba408d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Get feature names (assuming your feature names are stored in a list)\n",
    "feature_names = el\n",
    "\n",
    "# Sort the feature importances and select the top ten\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "top_ten_indices = sorted_idx[:10]\n",
    "top_ten_importances = importances[top_ten_indices]\n",
    "top_ten_feature_names = [feature_names[i] for i in top_ten_indices]\n",
    "\n",
    "# Create a bar graph for feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_ten_feature_names, top_ten_importances)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.title(\"Top Ten Feature Importance\")\n",
    "plt.gca().invert_yaxis()  # Invert the y-axis to display the most important feature at the top\n",
    "plt.savefig(\"Images/el_Xgboost_feat.png\")  # Save the plot to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af24bdef-94e6-482c-b071-0471a7064052",
   "metadata": {},
   "source": [
    "## Element Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c97b0-502e-4f48-ad8f-d82623badb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data\n",
    "X_frac = np.load(\"TrainingData/X_frac.npy\")\n",
    "y = np.load(\"TrainingData/y.npy\")\n",
    "\n",
    "frac = np.load(\"TrainingData/frac_col.npy\", allow_pickle=True) # allow_pickel = True when you input string data\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_frac, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850259f4-a57d-4bd1-82a3-1a17397a4a92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train an XGBoost model\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405c7c7-be84-4e34-98c1-f29d974585c7",
   "metadata": {},
   "source": [
    "### Saving and uploading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a30531-2f73-48ed-9d6c-7d38f325ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "with open(\"Models/frac_xgboost.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d7767f-aaa2-4fa5-9f3a-b470e5bc081d",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5789b3-d7b1-4759-8965-c1346ee82e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the saved file\n",
    "with open(\"Models/frac_xgboost.pkl\", \"rb\") as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "    \n",
    "# Now you can use the loaded model for predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE) as a measure of model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_text = f\"Mean Squared Error: {mse:.2f}\"\n",
    "\n",
    "# Create a graph for prediction vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Prediction vs. Actual Values\")\n",
    "\n",
    "# Add the MSE value as text in the graph\n",
    "plt.text(0.1, 0.9, mse_text, transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.savefig(\"Images/frac_xgboost.png\")  # Save the plot to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd56113-197d-4fcf-a3bb-84df3ff5d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the saved file\n",
    "with open(\"Models/frac_xgboost.pkl\", \"rb\") as model_file:\n",
    " model = pickle.load(model_file)\n",
    " \n",
    "# Now you can use the loaded model for predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE) as a measure of model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_text = f\"Mean Squared Error: {mse:.2f}\"\n",
    "\n",
    "# Calculate the R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r2_text = f\"R2 Score: {r2:.2f}\"\n",
    "\n",
    "# Calculate the slope and intercept of the line that best fits the data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(y_test.reshape(-1, 1), y_pred)\n",
    "slope, intercept = lr.coef_[0], lr.intercept_\n",
    "\n",
    "# Calculate the absolute differences\n",
    "diff = np.abs(y_test - y_pred)\n",
    "\n",
    "# Normalize the absolute differences\n",
    "norm = plt.Normalize(vmin=0, vmax=np.max(diff))\n",
    "\n",
    "# Create a colormap and use it to map the absolute differences to colors\n",
    "cmap = plt.cm.get_cmap('hot')\n",
    "colors = cmap(norm(diff))\n",
    "\n",
    "# Create a graph for prediction vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, c=colors, alpha=0.5)\n",
    "plt.plot(y_test, slope * y_test + intercept, linestyle='dashed')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Prediction vs. Actual Values\")\n",
    "\n",
    "# Add the MSE and R2 values as text in the graph\n",
    "plt.text(0.05, 0.7, mse_text, transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.05, 0.8, r2_text, transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.text(0.05, 0.9, \"Element Fraction\", transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "# Create a colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap='hot', norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm)\n",
    "cbar.ax.set_ylabel('|Actual - Predicted|', rotation=270, fontsize=15, labelpad=15)\n",
    "\n",
    "plt.savefig(\"Images/frac_xgboost.png\") # Save the plot to a file\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eae24a2-566a-4b7f-8cde-7cff87766fcd",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9682f-4070-45e4-b8c9-f69a37e781ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Get feature names (assuming your feature names are stored in a list)\n",
    "feature_names = frac\n",
    "\n",
    "# Sort the feature importances and select the top ten\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "top_ten_indices = sorted_idx[:10]\n",
    "top_ten_importances = importances[top_ten_indices]\n",
    "top_ten_feature_names = [feature_names[i] for i in top_ten_indices]\n",
    "\n",
    "# Create a bar graph for feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_ten_feature_names, top_ten_importances)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.title(\"Top Ten Feature Importance\")\n",
    "plt.gca().invert_yaxis()  # Invert the y-axis to display the most important feature at the top\n",
    "plt.savefig(\"Images/frac_Xgboost_feat.png\")  # Save the plot to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354b5a6-f54b-470e-8676-40ed07a72cff",
   "metadata": {},
   "source": [
    "## DFT Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f27643-fcec-4cbf-b2bd-979638d2fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "X_dft_atom = np.load(\"TrainingData/X_dft_atom.npy\")\n",
    "y = np.load(\"TrainingData/y_dft.npy\")\n",
    "_, feat_col_dft = X_dft_atom.shape\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dft_atom, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d13d4-8f2a-47a1-b27a-21d7976264f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Train an XGBoost model\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27938d4c-45e1-4461-bb65-512ea8e308d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "with open(\"Models/dft_xgboost.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf9d9b9-4b02-4a2f-aba6-b2530a0c9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the saved file\n",
    "with open(\"Models/dft_xgboost.pkl\", \"rb\") as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "    \n",
    "# Now you can use the loaded model for predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE) as a measure of model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_text = f\"Mean Squared Error: {mse:.2f}\"\n",
    "\n",
    "# Create a graph for prediction vs. actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Prediction vs. Actual Values\")\n",
    "\n",
    "# Add the MSE value as text in the graph\n",
    "plt.text(0.1, 0.9, mse_text, transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.savefig(\"Images/dft_xgboost.png\")  # Save the plot to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c52d688-2e7c-4551-a003-401534faa108",
   "metadata": {},
   "source": [
    "# DFT Based Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bce58d-2583-48e3-b92f-5f700e746507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dft = pd.read_excel(\"Database/DFTBased_Descriptors.xlsx\")\n",
    "df_dft = df_dft.dropna()\n",
    "df_dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61cce2b-16cc-4296-bb6e-c77022b465f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dft.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a509cf-a319-49c4-94cf-1c9357da99af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dft.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d6fe8-93aa-4b9d-a292-5432339e7028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key, val in enumerate(df_dft.columns):\n",
    "    print(f\"{key=}, {val=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319840f-8032-40c2-9c42-fbc58365fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_dft['formation_energy_per_atom']\n",
    "excluded = np.array(df_dft.columns[:21])\n",
    "\n",
    "X_dft_atom = df_dft.drop(excluded, axis=1)\n",
    "\n",
    "_, feat_col_dft = X_dft_atom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0df839-f6e5-48c0-8be2-b9a34a4a5b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk for later training\n",
    "np.save(\"TrainingData/X_dft_atom.npy\", X_dft_atom)\n",
    "np.save(\"TrainingData/y_dft.npy\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97238dac-fac6-4955-931d-74728cfa8953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "X_dft_atom = np.load(\"TrainingData/X_dft_atom.npy\")\n",
    "y = np.load(\"TrainingData/y_dft.npy\")\n",
    "_, feat_col_dft = X_dft_atom.shape\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dft_atom, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454b284-83b0-49e5-a9e0-294d87ccdaa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the input layer with the appropriate input shape\n",
    "input_layer = tf.keras.layers.Input(shape=(feat_col_dft,))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        input_layer,\n",
    "        tf.keras.layers.Dense(feat_col_dft),\n",
    "        tf.keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "history = model.fit(\n",
    "    np.array(X_train),\n",
    "    np.array(y_train),\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)],\n",
    "    batch_size=32\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e086e09-7d73-4fb2-8c3b-069770fe43e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the input layer with the appropriate input shape\n",
    "input_layer = tf.keras.layers.Input(shape=(feat_col_dft,))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        input_layer,\n",
    "        tf.keras.layers.Dense(128, activation='linear'),\n",
    "        tf.keras.layers.Dense(64, activation='linear'),\n",
    "        tf.keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "# # Add L2 regularization to the model\n",
    "# model.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "# model.add(tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "\n",
    "model.compile(loss='mean_squared_error',  # Switched to mean squared error\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "history = model.fit(\n",
    "    np.array(X_train),\n",
    "    np.array(y_train),\n",
    "    epochs=100,  # Increased the number of epochs\n",
    "    verbose=1,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)],  # Increased patience\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44cede-1d7f-4fa2-9516-06a23367c534",
   "metadata": {},
   "source": [
    "## One Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6238e4-100a-4105-9185-376307004e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_mult = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "results = {}\n",
    "\n",
    "def build_and_train_model(num_mult):\n",
    "\n",
    "    input_layer = tf.keras.layers.Input(shape=(feat_col_dft,))\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "            input_layer,\n",
    "            tf.keras.layers.Dense(int(feat_col_dft*num_mult)),\n",
    "            tf.keras.layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=\"adam\")\n",
    "\n",
    "    history = model.fit(\n",
    "        np.array(X_train),\n",
    "        np.array(y_train),\n",
    "        epochs=10,\n",
    "        verbose = 0,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)],\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    loss = history.history['val_loss'][-1]\n",
    "\n",
    "    return loss\n",
    "\n",
    "for num_mult in neuron_mult: # adding iteration\n",
    "    print(f\"Number of neuron: {feat_col_dft*num_mult}\")\n",
    "    loss = build_and_train_model(num_mult)\n",
    "    print(f\"Loss value: {loss}\")\n",
    "    print(\"-\"*50)\n",
    "    results[num_mult] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c4c430-3156-483c-9848-99b6190c0e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a JSON file\n",
    "# filepath = \"<file path>\"\n",
    "import json\n",
    "\n",
    "filename = \"OneLayer.json\"\n",
    "\n",
    "with open('DeepLearning'+'/'+filename, 'w') as json_file:\n",
    "    json.dump(results, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03092e0e-aaa0-4e3b-bd60-a5c0d5f7c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results from the JSON file\n",
    "with open('DeepLearning'+'/'+filename, 'r') as json_file:\n",
    "    results = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ae4990-55e1-4489-8c1c-619f4f398b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the values for the x-axis and y-axis from the results dictionary\n",
    "x_values = [num for num in neuron_mult]\n",
    "y_values = [results[str(num)] for num in neuron_mult]\n",
    "\n",
    "# Plot the loss vs. number of neurons\n",
    "plt.plot(x_values, y_values, marker='o')\n",
    "plt.xlabel(\"Number of Neurons\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title(\"Loss vs. Number of Neurons\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Save or display the plot\n",
    "plt.savefig('DeepLearning'+'/'+\"OneLayer.png\")  # Save the plot to a file\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aafe33-2927-40a2-832c-648c8e7a78b1",
   "metadata": {},
   "source": [
    "## Two layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b6fb9-cb2d-4716-9f1c-034e2a8cad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_mult = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "results = {}\n",
    "\n",
    "def build_and_train_model(num_mult):\n",
    "\n",
    "    input_layer = tf.keras.layers.Input(shape=(feat_col_dft,))\n",
    "\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer.adapt(np.array(X_dft_atom))\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "            input_layer,\n",
    "            normalizer,\n",
    "            tf.keras.layers.Dense(int(feat_col_dft, activation='relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=\"adam\")\n",
    "\n",
    "    history = model.fit(\n",
    "        np.array(X_train),\n",
    "        np.array(y_train),\n",
    "        epochs=10,\n",
    "        verbose = 0,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)],\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    loss = history.history['val_loss'][-1]\n",
    "\n",
    "    return loss\n",
    "\n",
    "for num_mult in neuron_mult: # adding iteration\n",
    "    print(f\"Number of neuron: {feat_col_dft*num_mult}\")\n",
    "    loss = build_and_train_model(num_mult)\n",
    "    print(f\"Loss value: {loss}\")\n",
    "    print(\"-\"*50)\n",
    "    results[num_mult] = loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
